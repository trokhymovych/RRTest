{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73756505",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/trokhymovych/opt/anaconda3/lib/python3.9/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# loading files with precalculated scores using test_model.py script\n",
    "results_rrla = joblib.load(\"../data/RRLA_results.data\")\n",
    "results_rrml = joblib.load(\"../data/RRML_results.data\")\n",
    "\n",
    "# data used for testing (10K samples)\n",
    "df = pd.read_csv(\"../data/test_all_users_sample.csv\")\n",
    "\n",
    "\n",
    "def precision_at_recall(y_true, y_scores, recall_thr=0.75):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "    idx = np.where(recall > recall_thr)[0][-1]\n",
    "    return precision[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b7f55a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining variables for metrics saving for different configurations\n",
    "names, model_error_rates, model_mean_time = [], [], []\n",
    "model_auc, model_auc_anon, model_auc_auth, model_pr_r_25 = [], [], [], []  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c69655e",
   "metadata": {},
   "source": [
    "## Multilingual stable model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d393fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for metrics calculation from dumped results:\n",
    "\n",
    "def calculate_metrics(result_dump, true_data, model_name=\"\", verbose=False):\n",
    "    \n",
    "    # processing data used for further metrics calculation\n",
    "    true_dict = {r: l for r, l in zip(true_data.revision_id, true_data.revision_is_identity_reverted)}\n",
    "    is_anon_dict = {r: l for r, l in zip(true_data.revision_id, true_data.user_is_anonymous)}\n",
    "\n",
    "    error_counts = 0\n",
    "    rev_ids = []\n",
    "    true_labels = []\n",
    "    times = []\n",
    "    scores = []\n",
    "    is_anon = []\n",
    "\n",
    "    for r in result_dump:\n",
    "        if r[3] is not None:\n",
    "            error_counts+=1\n",
    "            continue\n",
    "        else:\n",
    "            rev_ids.append(r[0])\n",
    "            true_labels.append(true_dict[r[0]])\n",
    "            times.append(r[2])\n",
    "            scores.append(r[1].probability)\n",
    "            is_anon.append(is_anon_dict[r[0]])\n",
    "\n",
    "    anon_auc = roc_auc_score(\n",
    "        [l for l, g in zip(true_labels, is_anon) if g], \n",
    "        [l for l, g in zip(scores, is_anon) if g]\n",
    "    )\n",
    "    all_auc = roc_auc_score(\n",
    "        [l for l, g in zip(true_labels, is_anon) if not g], \n",
    "        [l for l, g in zip(scores, is_anon) if not g]\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        print(model_name)\n",
    "        print(\"AUC score: \", roc_auc_score(true_labels, scores))\n",
    "        print(\"AUC authorised: \", all_auc)\n",
    "        print(\"AUC anon: \", anon_auc)\n",
    "        print(\"Precision at Recall 0.75: \", precision_at_recall(true_labels, scores, recall_thr=0.75))\n",
    "        print()\n",
    "        print(\"Error rate: \", error_counts / len(true_data))\n",
    "        print()\n",
    "        print(\"Time distribution (20 simultanious requests on M2 CPU):\\n\", pd.Series(times).describe())\n",
    "    \n",
    "    return error_counts / len(true_data), np.mean(times), roc_auc_score(true_labels, scores), \\\n",
    "        anon_auc, all_auc, precision_at_recall(true_labels, scores, recall_thr=0.75)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fdf4a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multilingual RR\n",
      "AUC score:  0.8785292720192289\n",
      "AUC authorised:  0.7827275359463663\n",
      "AUC anon:  0.7811554112293784\n",
      "Precision at Recall 0.75:  0.28831312017640576\n",
      "\n",
      "Error rate:  0.0063\n",
      "\n",
      "Time distribution (20 simultanious requests on M2 CPU):\n",
      " count    9937.000000\n",
      "mean        3.130323\n",
      "std         3.106043\n",
      "min         0.366666\n",
      "25%         1.208242\n",
      "50%         2.317784\n",
      "75%         4.228241\n",
      "max       140.010956\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Multilingual RR\"\n",
    "\n",
    "error_rate, mean_time, auc, auc_anon, auc_auth, pr_r_25 = \\\n",
    "    calculate_metrics(results_rrml, df, model_name=model_name, verbose=True)\n",
    "\n",
    "model_error_rates.append(error_rate)\n",
    "model_mean_time.append(mean_time)\n",
    "model_auc.append(auc)\n",
    "model_auc_anon.append(auc_anon)\n",
    "model_auc_auth.append(auc_auth)\n",
    "model_pr_r_25.append(pr_r_25)\n",
    "names.append(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52afc7e8",
   "metadata": {},
   "source": [
    "## Language-agnostic model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e4653f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language-agnostic RR\n",
      "AUC score:  0.8653014728780128\n",
      "AUC authorised:  0.7803407351260373\n",
      "AUC anon:  0.6999705047693502\n",
      "Precision at Recall 0.75:  0.25801677355698077\n",
      "\n",
      "Error rate:  0.0061\n",
      "\n",
      "Time distribution (20 simultanious requests on M2 CPU):\n",
      " count    9939.000000\n",
      "mean        0.538154\n",
      "std         0.230301\n",
      "min         0.322874\n",
      "25%         0.387910\n",
      "50%         0.450326\n",
      "75%         0.608941\n",
      "max         7.007304\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Language-agnostic RR\"\n",
    "\n",
    "error_rate, mean_time, auc, auc_anon, auc_auth, pr_r_25 = \\\n",
    "    calculate_metrics(results_rrla, df, model_name=model_name, verbose=True)\n",
    "\n",
    "model_error_rates.append(error_rate)\n",
    "model_mean_time.append(mean_time)\n",
    "model_auc.append(auc)\n",
    "model_auc_anon.append(auc_anon)\n",
    "model_auc_auth.append(auc_auth)\n",
    "model_pr_r_25.append(pr_r_25)\n",
    "names.append(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aa0045",
   "metadata": {},
   "source": [
    "# ORES\n",
    "\n",
    "We have all the ORES scores precalculated -> use tham for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50589a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:\n",
      "Revision_id: 1095887742, Score: 0.0080322643704155\n"
     ]
    }
   ],
   "source": [
    "# Load ores scores: \n",
    "ores_scores = pd.read_csv(\"../data/test_ores_scores_full_test.csv\")\n",
    "ores_scores_dict = {r: i for r, i in zip(ores_scores.revision_id, ores_scores.ores_pred) if not pd.isnull(i)}\n",
    "\n",
    "# Example of one record:\n",
    "random_key = np.random.choice(list(ores_scores_dict.keys()))\n",
    "\n",
    "print(\"Example:\")\n",
    "print(f\"Revision_id: {random_key}, Score: {ores_scores_dict[random_key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fbbed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_dict = {r: l for r, l in zip(df.revision_id, df.revision_is_identity_reverted)}\n",
    "is_anon_dict = {r: l for r, l in zip(df.revision_id, df.user_is_anonymous)}\n",
    "\n",
    "error_counts = 0\n",
    "\n",
    "rev_ids = []\n",
    "true_labels = []\n",
    "times = []\n",
    "scores = []\n",
    "is_anon = []\n",
    "\n",
    "ores_is_available = []\n",
    "\n",
    "for r in results_rrla:\n",
    "    ores_is_available.append(r[0] in ores_scores_dict)\n",
    "    if not ores_is_available[-1]: \n",
    "        continue\n",
    "    if r[3] is not None:\n",
    "        error_counts+=1\n",
    "        continue\n",
    "    else:\n",
    "        rev_ids.append(r[0])\n",
    "        true_labels.append(true_dict[r[0]])\n",
    "        times.append(r[2])\n",
    "        scores.append(ores_scores_dict.get(r[0], 0))\n",
    "        is_anon.append(is_anon_dict[r[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "101c9dfd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score:  0.840866173596575\n",
      "AUC authorised:  0.7095457138226181\n",
      "AUC anon:  0.6823470085470085\n",
      "Precision at Recall 0.75:  0.23596059113300494\n"
     ]
    }
   ],
   "source": [
    "print(\"AUC score: \", roc_auc_score(true_labels, scores))\n",
    "\n",
    "anon_auc = roc_auc_score(\n",
    "    [l for l, g in zip(true_labels, is_anon) if g], \n",
    "    [l for l, g in zip(scores, is_anon) if g]\n",
    ")\n",
    "all_auc = roc_auc_score(\n",
    "    [l for l, g in zip(true_labels, is_anon) if not g], \n",
    "    [l for l, g in zip(scores, is_anon) if not g]\n",
    ")\n",
    "\n",
    "print(\"AUC authorised: \", all_auc)\n",
    "print(\"AUC anon: \", anon_auc)\n",
    "print(\"Precision at Recall 0.75: \", precision_at_recall(true_labels, scores, recall_thr=0.75))\n",
    "\n",
    "names.append(\"ORES\")\n",
    "model_error_rates.append(None)\n",
    "model_mean_time.append(None)\n",
    "model_auc.append(roc_auc_score(true_labels, scores))\n",
    "model_auc_anon.append(anon_auc)\n",
    "model_auc_auth.append(all_auc)\n",
    "model_pr_r_25.append(precision_at_recall(true_labels, scores, recall_thr=0.75))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8d0d68",
   "metadata": {},
   "source": [
    "## Final comparison table: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4232aff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>model_processing_time</th>\n",
       "      <th>AUC</th>\n",
       "      <th>AUC anonymous</th>\n",
       "      <th>AUC authorised</th>\n",
       "      <th>Pr@R0.25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Multilingual RR</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>3.130323</td>\n",
       "      <td>0.878529</td>\n",
       "      <td>0.781155</td>\n",
       "      <td>0.782728</td>\n",
       "      <td>0.288313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Language-agnostic RR</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.538154</td>\n",
       "      <td>0.865301</td>\n",
       "      <td>0.699971</td>\n",
       "      <td>0.780341</td>\n",
       "      <td>0.258017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ORES</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.840866</td>\n",
       "      <td>0.682347</td>\n",
       "      <td>0.709546</td>\n",
       "      <td>0.235961</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  model  error_rate  model_processing_time       AUC  \\\n",
       "0       Multilingual RR      0.0063               3.130323  0.878529   \n",
       "1  Language-agnostic RR      0.0061               0.538154  0.865301   \n",
       "2                  ORES         NaN                    NaN  0.840866   \n",
       "\n",
       "   AUC anonymous  AUC authorised  Pr@R0.25  \n",
       "0       0.781155        0.782728  0.288313  \n",
       "1       0.699971        0.780341  0.258017  \n",
       "2       0.682347        0.709546  0.235961  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\n",
    "    \"model\": names,\n",
    "    \"error_rate\": model_error_rates, \n",
    "    \"model_processing_time\": model_mean_time,\n",
    "    \"AUC\": model_auc,\n",
    "    \"AUC anonymous\":model_auc_anon,\n",
    "    \"AUC authorised\" :model_auc_auth,\n",
    "    \"Pr@R0.25\": model_pr_r_25,\n",
    "})\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
